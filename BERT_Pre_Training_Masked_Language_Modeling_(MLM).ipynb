{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyORnCgFqE/2MG+sC4tRSgl1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DJ-Manjaray/BERT-Practices/blob/main/BERT_Pre_Training_Masked_Language_Modeling_(MLM).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "UaNz-MIw9YTG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "76pj7oFO28ge"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, BertModel, BertConfig"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixyFro159cyB",
        "outputId": "c0143a4c-df5f-4bc9-c3b0-d34b6ded8744"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = (\"BERT is an open source machine learning framework for natural language processing (NLP). BERT is designed to help computers understand the meaning of ambiguous language in text by using surrounding text to establish context. The BERT framework was pre-trained using text from Wikipedia and can be fine-tuned with question and answer datasets.\"\n",
        "\n",
        "\"BERT, which stands for Bidirectional Encoder Representations from Transformers, is based on Transformers, a deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their connection. In NLP, this process is called attention.\"\n",
        "\n",
        "\"Historically, language models could only read text input sequentially -- either left-to-right or right-to-left -- but couldn't do both at the same time. BERT is different because it is designed to read in both directions at once. This capability, enabled by the introduction of Transformers, is known as bidirectionality.\"\n",
        "\n",
        "\"Using this bidirectional capability, BERT is pre-trained on two different, but related, NLP tasks: Masked Language Modeling and Next Sentence Prediction.\"\n",
        "\n",
        "\"The objective of Masked Language Model (MLM) training is to hide a word in a sentence and then have the program predict what word has been hidden (masked) based on the hidden word's context. The objective of Next Sentence Prediction training is to have the program predict whether two given sentences have a logical, sequential connection or whether their relationship is simply random.\")"
      ],
      "metadata": {
        "id": "0uyoUNoD-ESr"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(text, return_tensors='pt')\n",
        "inputs.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTw-n1K7_KGA",
        "outputId": "b17013b1-b726-4d83-b9e4-6a2c543a370e"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrxDvoPQAA1w",
        "outputId": "3aff64a8-3409-4b60-9e07-962c45651d49"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101, 14324,  2003,  2019,  2330,  3120,  3698,  4083,  7705,  2005,\n",
              "          3019,  2653,  6364,  1006, 17953,  2361,  1007,  1012, 14324,  2003,\n",
              "          2881,  2000,  2393,  7588,  3305,  1996,  3574,  1997, 20080,  2653,\n",
              "          1999,  3793,  2011,  2478,  4193,  3793,  2000,  5323,  6123,  1012,\n",
              "          1996, 14324,  7705,  2001,  3653,  1011,  4738,  2478,  3793,  2013,\n",
              "         16948,  1998,  2064,  2022,  2986,  1011, 15757,  2007,  3160,  1998,\n",
              "          3437,  2951, 13462,  2015,  1012, 14324,  1010,  2029,  4832,  2005,\n",
              "          7226,  7442,  7542,  2389,  4372, 16044,  2099, 15066,  2013, 19081,\n",
              "          1010,  2003,  2241,  2006, 19081,  1010,  1037,  2784,  4083,  2944,\n",
              "          1999,  2029,  2296,  6434,  5783,  2003,  4198,  2000,  2296,  7953,\n",
              "          5783,  1010,  1998,  1996,  3635,  8613,  2090,  2068,  2024,  8790,\n",
              "          3973, 10174,  2241,  2588,  2037,  4434,  1012,  1999, 17953,  2361,\n",
              "          1010,  2023,  2832,  2003,  2170,  3086,  1012,  7145,  1010,  2653,\n",
              "          4275,  2071,  2069,  3191,  3793,  7953, 25582,  2135,  1011,  1011,\n",
              "          2593,  2187,  1011,  2000,  1011,  2157,  2030,  2157,  1011,  2000,\n",
              "          1011,  2187,  1011,  1011,  2021,  2481,  1005,  1056,  2079,  2119,\n",
              "          2012,  1996,  2168,  2051,  1012, 14324,  2003,  2367,  2138,  2009,\n",
              "          2003,  2881,  2000,  3191,  1999,  2119,  7826,  2012,  2320,  1012,\n",
              "          2023, 10673,  1010,  9124,  2011,  1996,  4955,  1997, 19081,  1010,\n",
              "          2003,  2124,  2004,  7226,  7442,  7542, 23732,  1012,  2478,  2023,\n",
              "          7226,  7442,  7542,  2389, 10673,  1010, 14324,  2003,  3653,  1011,\n",
              "          4738,  2006,  2048,  2367,  1010,  2021,  3141,  1010, 17953,  2361,\n",
              "          8518,  1024, 16520,  2653, 11643,  1998,  2279,  6251, 17547,  1012,\n",
              "          1996,  7863,  1997, 16520,  2653,  2944,  1006, 19875,  2213,  1007,\n",
              "          2731,  2003,  2000,  5342,  1037,  2773,  1999,  1037,  6251,  1998,\n",
              "          2059,  2031,  1996,  2565, 16014,  2054,  2773,  2038,  2042,  5023,\n",
              "          1006, 16520,  1007,  2241,  2006,  1996,  5023,  2773,  1005,  1055,\n",
              "          6123,  1012,  1996,  7863,  1997,  2279,  6251, 17547,  2731,  2003,\n",
              "          2000,  2031,  1996,  2565, 16014,  3251,  2048,  2445, 11746,  2031,\n",
              "          1037, 11177,  1010, 25582,  4434,  2030,  3251,  2037,  3276,  2003,\n",
              "          3432,  6721,  1012,   102]])"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.token_type_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ov6Z6wvBAGhe",
        "outputId": "ab83ae31-2042-4333-b4a2-5a0a39112bce"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.attention_mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1I6hTrdATjR",
        "outputId": "3671ed60-7a4b-4aa5-b1f0-ddf30f268fa5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs['labels'] = inputs.input_ids.detach().clone()\n",
        "inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNqyBQeDAjTt",
        "outputId": "71b63024-dab0-4ea6-bffc-541d58694dbb"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[  101, 14324,  2003,  2019,  2330,  3120,  3698,  4083,  7705,  2005,\n",
              "          3019,  2653,  6364,  1006, 17953,  2361,  1007,  1012, 14324,  2003,\n",
              "          2881,  2000,  2393,  7588,  3305,  1996,  3574,  1997, 20080,  2653,\n",
              "          1999,  3793,  2011,  2478,  4193,  3793,  2000,  5323,  6123,  1012,\n",
              "          1996, 14324,  7705,  2001,  3653,  1011,  4738,  2478,  3793,  2013,\n",
              "         16948,  1998,  2064,  2022,  2986,  1011, 15757,  2007,  3160,  1998,\n",
              "          3437,  2951, 13462,  2015,  1012, 14324,  1010,  2029,  4832,  2005,\n",
              "          7226,  7442,  7542,  2389,  4372, 16044,  2099, 15066,  2013,   103,\n",
              "          1010,  2003,  2241,  2006, 19081,  1010,  1037,  2784,  4083,  2944,\n",
              "          1999,  2029,  2296,  6434,  5783,  2003,  4198,  2000,  2296,  7953,\n",
              "          5783,  1010,  1998,  1996,  3635,  8613,  2090,  2068,  2024,  8790,\n",
              "          3973, 10174,  2241,  2588,  2037,  4434,  1012,  1999, 17953,  2361,\n",
              "          1010,  2023,  2832,  2003,  2170,  3086,  1012,  7145,  1010,  2653,\n",
              "          4275,  2071,  2069,  3191,  3793,  7953, 25582,  2135,  1011,  1011,\n",
              "          2593,  2187,  1011,  2000,  1011,  2157,  2030,  2157,  1011,  2000,\n",
              "          1011,  2187,  1011,  1011,  2021,  2481,  1005,  1056,  2079,  2119,\n",
              "          2012,  1996,  2168,  2051,  1012, 14324,  2003,  2367,  2138,  2009,\n",
              "          2003,  2881,  2000,  3191,  1999,  2119,  7826,  2012,  2320,  1012,\n",
              "          2023, 10673,  1010,  9124,  2011,  1996,  4955,  1997, 19081,  1010,\n",
              "          2003,  2124,  2004,  7226,  7442,  7542, 23732,  1012,  2478,  2023,\n",
              "          7226,  7442,  7542,  2389, 10673,  1010,   103,  2003,  3653,  1011,\n",
              "          4738,  2006,  2048,  2367,  1010,  2021,  3141,  1010, 17953,  2361,\n",
              "          8518,  1024, 16520,  2653, 11643,  1998,  2279,  6251, 17547,  1012,\n",
              "          1996,  7863,  1997, 16520,  2653,  2944,  1006, 19875,  2213,  1007,\n",
              "          2731,  2003,  2000,  5342,  1037,  2773,  1999,  1037,  6251,  1998,\n",
              "          2059,  2031,  1996,  2565, 16014,  2054,  2773,  2038,  2042,  5023,\n",
              "          1006, 16520,  1007,  2241,  2006,  1996,  5023,  2773,  1005,  1055,\n",
              "          6123,  1012,  1996,  7863,  1997,  2279,  6251, 17547,  2731,  2003,\n",
              "          2000,  2031,  1996,  2565, 16014,  3251,  2048,  2445, 11746,  2031,\n",
              "          1037, 11177,  1010, 25582,  4434,  2030,  3251,  2037,  3276,  2003,\n",
              "          3432,  6721,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[  101, 14324,  2003,  2019,  2330,  3120,  3698,  4083,  7705,  2005,\n",
              "          3019,  2653,  6364,  1006, 17953,  2361,  1007,  1012, 14324,  2003,\n",
              "          2881,  2000,  2393,  7588,  3305,  1996,  3574,  1997, 20080,  2653,\n",
              "          1999,  3793,  2011,  2478,  4193,  3793,  2000,  5323,  6123,  1012,\n",
              "          1996, 14324,  7705,  2001,  3653,  1011,  4738,  2478,  3793,  2013,\n",
              "         16948,  1998,  2064,  2022,  2986,  1011, 15757,  2007,  3160,  1998,\n",
              "          3437,  2951, 13462,  2015,  1012, 14324,  1010,  2029,  4832,  2005,\n",
              "          7226,  7442,  7542,  2389,  4372, 16044,  2099, 15066,  2013,   103,\n",
              "          1010,  2003,  2241,  2006, 19081,  1010,  1037,  2784,  4083,  2944,\n",
              "          1999,  2029,  2296,  6434,  5783,  2003,  4198,  2000,  2296,  7953,\n",
              "          5783,  1010,  1998,  1996,  3635,  8613,  2090,  2068,  2024,  8790,\n",
              "          3973, 10174,  2241,  2588,  2037,  4434,  1012,  1999, 17953,  2361,\n",
              "          1010,  2023,  2832,  2003,  2170,  3086,  1012,  7145,  1010,  2653,\n",
              "          4275,  2071,  2069,  3191,  3793,  7953, 25582,  2135,  1011,  1011,\n",
              "          2593,  2187,  1011,  2000,  1011,  2157,  2030,  2157,  1011,  2000,\n",
              "          1011,  2187,  1011,  1011,  2021,  2481,  1005,  1056,  2079,  2119,\n",
              "          2012,  1996,  2168,  2051,  1012, 14324,  2003,  2367,  2138,  2009,\n",
              "          2003,  2881,  2000,  3191,  1999,  2119,  7826,  2012,  2320,  1012,\n",
              "          2023, 10673,  1010,  9124,  2011,  1996,  4955,  1997, 19081,  1010,\n",
              "          2003,  2124,  2004,  7226,  7442,  7542, 23732,  1012,  2478,  2023,\n",
              "          7226,  7442,  7542,  2389, 10673,  1010,   103,  2003,  3653,  1011,\n",
              "          4738,  2006,  2048,  2367,  1010,  2021,  3141,  1010, 17953,  2361,\n",
              "          8518,  1024, 16520,  2653, 11643,  1998,  2279,  6251, 17547,  1012,\n",
              "          1996,  7863,  1997, 16520,  2653,  2944,  1006, 19875,  2213,  1007,\n",
              "          2731,  2003,  2000,  5342,  1037,  2773,  1999,  1037,  6251,  1998,\n",
              "          2059,  2031,  1996,  2565, 16014,  2054,  2773,  2038,  2042,  5023,\n",
              "          1006, 16520,  1007,  2241,  2006,  1996,  5023,  2773,  1005,  1055,\n",
              "          6123,  1012,  1996,  7863,  1997,  2279,  6251, 17547,  2731,  2003,\n",
              "          2000,  2031,  1996,  2565, 16014,  3251,  2048,  2445, 11746,  2031,\n",
              "          1037, 11177,  1010, 25582,  4434,  2030,  3251,  2037,  3276,  2003,\n",
              "          3432,  6721,  1012,   102]])}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand = torch.rand(inputs.input_ids.shape)\n",
        "rand"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-YJuEWsFA3_3",
        "outputId": "4b3ed4ae-f5fc-46a8-eac5-66434e1f49ab"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.2593, 0.5953, 0.1092, 0.3616, 0.9191, 0.7602, 0.2828, 0.3544, 0.6703,\n",
              "         0.8413, 0.6071, 0.5969, 0.8181, 0.0845, 0.7725, 0.6637, 0.8535, 0.5365,\n",
              "         0.1595, 0.9534, 0.8764, 0.4509, 0.5654, 0.2474, 0.3124, 0.3676, 0.4600,\n",
              "         0.6903, 0.7297, 0.3792, 0.0938, 0.9095, 0.7335, 0.5713, 0.4749, 0.6393,\n",
              "         0.5409, 0.5935, 0.9659, 0.6411, 0.4194, 0.4390, 0.5831, 0.8852, 0.7797,\n",
              "         0.5904, 0.0456, 0.9076, 0.5134, 0.0423, 0.2761, 0.8279, 0.0642, 0.4280,\n",
              "         0.7568, 0.3554, 0.4243, 0.1097, 0.7952, 0.7994, 0.4854, 0.1570, 0.2414,\n",
              "         0.8921, 0.7881, 0.7165, 0.7512, 0.4728, 0.9416, 0.5477, 0.8476, 0.2483,\n",
              "         0.9728, 0.5492, 0.7185, 0.8530, 0.7051, 0.2152, 0.0684, 0.2448, 0.6838,\n",
              "         0.3885, 0.0104, 0.9469, 0.6693, 0.2179, 0.7497, 0.2379, 0.0030, 0.0089,\n",
              "         0.3272, 0.8721, 0.9135, 0.1497, 0.0532, 0.1041, 0.8041, 0.8083, 0.8850,\n",
              "         0.5733, 0.5523, 0.6481, 0.3930, 0.8381, 0.8101, 0.2649, 0.6598, 0.9785,\n",
              "         0.3844, 0.3989, 0.2326, 0.5644, 0.9551, 0.3217, 0.0015, 0.7500, 0.6714,\n",
              "         0.5733, 0.1352, 0.4707, 0.4388, 0.3034, 0.5027, 0.8378, 0.0662, 0.2438,\n",
              "         0.0271, 0.8259, 0.9452, 0.8784, 0.9416, 0.2212, 0.7293, 0.5347, 0.0237,\n",
              "         0.2771, 0.1846, 0.6847, 0.9815, 0.0837, 0.3378, 0.1086, 0.3478, 0.7566,\n",
              "         0.8954, 0.7219, 0.8630, 0.4467, 0.6951, 0.1413, 0.1815, 0.4496, 0.2953,\n",
              "         0.2589, 0.8909, 0.0825, 0.4604, 0.9595, 0.9366, 0.5571, 0.0442, 0.6314,\n",
              "         0.4312, 0.2054, 0.3101, 0.1673, 0.9202, 0.8936, 0.7941, 0.9063, 0.1159,\n",
              "         0.7452, 0.4939, 0.7702, 0.6914, 0.3036, 0.2906, 0.5925, 0.5558, 0.1694,\n",
              "         0.0226, 0.0924, 0.1526, 0.5656, 0.3514, 0.4055, 0.9388, 0.3960, 0.9015,\n",
              "         0.7565, 0.8443, 0.4070, 0.9430, 0.6479, 0.8095, 0.7043, 0.5729, 0.2591,\n",
              "         0.5755, 0.1487, 0.8318, 0.4515, 0.8898, 0.4660, 0.3006, 0.5138, 0.4287,\n",
              "         0.0200, 0.5495, 0.8238, 0.4807, 0.6016, 0.3596, 0.4891, 0.0200, 0.8850,\n",
              "         0.0144, 0.1643, 0.7417, 0.1904, 0.5156, 0.5539, 0.0691, 0.2796, 0.1257,\n",
              "         0.1884, 0.7568, 0.7002, 0.4634, 0.0265, 0.3919, 0.5686, 0.7452, 0.9998,\n",
              "         0.8324, 0.2864, 0.0341, 0.5806, 0.4266, 0.3802, 0.8549, 0.9113, 0.2122,\n",
              "         0.5797, 0.5589, 0.4069, 0.1059, 0.8195, 0.9092, 0.6081, 0.9908, 0.5461,\n",
              "         0.2437, 0.0958, 0.0092, 0.8406, 0.4468, 0.5232, 0.3911, 0.0644, 0.4945,\n",
              "         0.3824, 0.5990, 0.1292, 0.6876, 0.6272, 0.6156, 0.6454, 0.4866, 0.8328,\n",
              "         0.0102, 0.2112, 0.8430, 0.7259, 0.6100, 0.7671, 0.4281, 0.1188, 0.4249,\n",
              "         0.9018, 0.4207, 0.9233, 0.8504, 0.6586, 0.4004, 0.2720, 0.1720, 0.1597,\n",
              "         0.3557, 0.5650, 0.9378, 0.0172, 0.8202, 0.6591, 0.6199, 0.0379, 0.0388,\n",
              "         0.9326, 0.9502, 0.4239, 0.5594, 0.3445, 0.9871, 0.4707]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rand.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-x4HpdNBLVc",
        "outputId": "4c41b43d-0f65-4e16-9a2a-9eae63e9e75a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 304])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(inputs.input_ids != 101)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNYYAzquFYTw",
        "outputId": "0fce3631-7652-42c3-d4e8-fe9daae8501e"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True]])"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(inputs.input_ids != 102)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gF1G1ygyFZJ4",
        "outputId": "c0fbd031-1813-4e29-9ef9-52a085aaca2e"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "          True,  True,  True, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_array = (rand < 0.15) * (inputs.input_ids != 101) * (inputs.input_ids != 102)\n",
        "mask_array"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swO0ugu6BTDY",
        "outputId": "c2cfdfec-4f74-45f3-fec3-1f7ada10115f"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False, False,  True, False, False, False, False, False, False, False,\n",
              "         False, False, False,  True, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "          True, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False,  True, False, False,  True,\n",
              "         False, False,  True, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False,  True, False,\n",
              "         False, False,  True, False, False, False, False, False,  True,  True,\n",
              "         False, False, False,  True,  True,  True, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False,  True, False, False, False,  True, False,\n",
              "         False, False, False, False,  True, False,  True, False, False, False,\n",
              "         False, False, False, False,  True, False, False, False, False,  True,\n",
              "         False,  True, False, False, False, False, False, False, False,  True,\n",
              "         False, False, False, False, False,  True, False, False, False, False,\n",
              "          True, False, False, False, False, False, False, False, False, False,\n",
              "          True, False, False, False, False, False, False, False, False, False,\n",
              "          True,  True, False, False, False, False, False, False, False, False,\n",
              "         False, False, False, False, False, False, False, False, False,  True,\n",
              "         False, False, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False,  True, False,  True, False, False, False,\n",
              "         False, False,  True, False,  True, False, False, False, False,  True,\n",
              "         False, False, False, False, False, False,  True, False, False, False,\n",
              "         False, False, False, False, False, False,  True, False, False, False,\n",
              "         False, False, False,  True,  True, False, False, False, False,  True,\n",
              "         False, False, False,  True, False, False, False, False, False, False,\n",
              "          True, False, False, False, False, False, False,  True, False, False,\n",
              "         False, False, False, False, False, False, False, False, False, False,\n",
              "         False,  True, False, False, False,  True,  True, False, False, False,\n",
              "         False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_array[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVqCbGdxBTA5",
        "outputId": "872a4efb-89c3-4000-faa1-f19920b140ab"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([False, False,  True, False, False, False, False, False, False, False,\n",
              "        False, False, False,  True, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False, False,\n",
              "         True, False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False,  True, False, False,  True,\n",
              "        False, False,  True, False, False, False, False,  True, False, False,\n",
              "        False, False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False,  True, False,\n",
              "        False, False,  True, False, False, False, False, False,  True,  True,\n",
              "        False, False, False,  True,  True,  True, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False,  True, False, False, False,  True, False,\n",
              "        False, False, False, False,  True, False,  True, False, False, False,\n",
              "        False, False, False, False,  True, False, False, False, False,  True,\n",
              "        False,  True, False, False, False, False, False, False, False,  True,\n",
              "        False, False, False, False, False,  True, False, False, False, False,\n",
              "         True, False, False, False, False, False, False, False, False, False,\n",
              "         True, False, False, False, False, False, False, False, False, False,\n",
              "         True,  True, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False, False, False, False, False, False,  True,\n",
              "        False, False, False, False, False, False, False,  True, False, False,\n",
              "        False, False, False, False,  True, False,  True, False, False, False,\n",
              "        False, False,  True, False,  True, False, False, False, False,  True,\n",
              "        False, False, False, False, False, False,  True, False, False, False,\n",
              "        False, False, False, False, False, False,  True, False, False, False,\n",
              "        False, False, False,  True,  True, False, False, False, False,  True,\n",
              "        False, False, False,  True, False, False, False, False, False, False,\n",
              "         True, False, False, False, False, False, False,  True, False, False,\n",
              "        False, False, False, False, False, False, False, False, False, False,\n",
              "        False,  True, False, False, False,  True,  True, False, False, False,\n",
              "        False, False, False, False])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask_array[0].nonzero()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z708TM0PFueY",
        "outputId": "fd74eeb1-2fd9-447f-cb02-f6d49831a2c7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  2],\n",
              "        [ 13],\n",
              "        [ 30],\n",
              "        [ 46],\n",
              "        [ 49],\n",
              "        [ 52],\n",
              "        [ 57],\n",
              "        [ 78],\n",
              "        [ 82],\n",
              "        [ 88],\n",
              "        [ 89],\n",
              "        [ 93],\n",
              "        [ 94],\n",
              "        [ 95],\n",
              "        [114],\n",
              "        [118],\n",
              "        [124],\n",
              "        [126],\n",
              "        [134],\n",
              "        [139],\n",
              "        [141],\n",
              "        [149],\n",
              "        [155],\n",
              "        [160],\n",
              "        [170],\n",
              "        [180],\n",
              "        [181],\n",
              "        [199],\n",
              "        [207],\n",
              "        [214],\n",
              "        [216],\n",
              "        [222],\n",
              "        [224],\n",
              "        [229],\n",
              "        [236],\n",
              "        [246],\n",
              "        [253],\n",
              "        [254],\n",
              "        [259],\n",
              "        [263],\n",
              "        [270],\n",
              "        [277],\n",
              "        [291],\n",
              "        [295],\n",
              "        [296]])"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selection = torch.flatten(mask_array[0].nonzero()).tolist()\n",
        "selection"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KqNOCJYF4-l",
        "outputId": "81f9c686-3850-4e49-e9be-f2ba60a13652"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2,\n",
              " 13,\n",
              " 30,\n",
              " 46,\n",
              " 49,\n",
              " 52,\n",
              " 57,\n",
              " 78,\n",
              " 82,\n",
              " 88,\n",
              " 89,\n",
              " 93,\n",
              " 94,\n",
              " 95,\n",
              " 114,\n",
              " 118,\n",
              " 124,\n",
              " 126,\n",
              " 134,\n",
              " 139,\n",
              " 141,\n",
              " 149,\n",
              " 155,\n",
              " 160,\n",
              " 170,\n",
              " 180,\n",
              " 181,\n",
              " 199,\n",
              " 207,\n",
              " 214,\n",
              " 216,\n",
              " 222,\n",
              " 224,\n",
              " 229,\n",
              " 236,\n",
              " 246,\n",
              " 253,\n",
              " 254,\n",
              " 259,\n",
              " 263,\n",
              " 270,\n",
              " 277,\n",
              " 291,\n",
              " 295,\n",
              " 296]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs.input_ids[0, selection] = 103\n",
        "inputs.input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G2TPWD1GMLV",
        "outputId": "cc09f26e-4210-43f9-e416-d0aa97a79a43"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  101, 14324,   103,  2019,  2330,  3120,  3698,  4083,  7705,  2005,\n",
              "          3019,  2653,  6364,   103, 17953,  2361,  1007,  1012, 14324,  2003,\n",
              "          2881,  2000,  2393,  7588,  3305,  1996,  3574,  1997, 20080,  2653,\n",
              "           103,  3793,  2011,  2478,  4193,  3793,  2000,  5323,  6123,  1012,\n",
              "          1996, 14324,  7705,  2001,  3653,  1011,   103,  2478,  3793,   103,\n",
              "         16948,  1998,   103,  2022,  2986,  1011, 15757,   103,  3160,  1998,\n",
              "          3437,  2951, 13462,  2015,  1012, 14324,  1010,  2029,  4832,  2005,\n",
              "          7226,  7442,  7542,  2389,  4372, 16044,  2099, 15066,   103,   103,\n",
              "          1010,  2003,   103,  2006, 19081,  1010,  1037,  2784,   103,   103,\n",
              "          1999,  2029,  2296,   103,   103,   103,  4198,  2000,  2296,  7953,\n",
              "          5783,  1010,  1998,  1996,  3635,  8613,  2090,  2068,  2024,  8790,\n",
              "          3973, 10174,  2241,  2588,   103,  4434,  1012,  1999,   103,  2361,\n",
              "          1010,  2023,  2832,  2003,   103,  3086,   103,  7145,  1010,  2653,\n",
              "          4275,  2071,  2069,  3191,   103,  7953, 25582,  2135,  1011,   103,\n",
              "          2593,   103,  1011,  2000,  1011,  2157,  2030,  2157,  1011,   103,\n",
              "          1011,  2187,  1011,  1011,  2021,   103,  1005,  1056,  2079,  2119,\n",
              "           103,  1996,  2168,  2051,  1012, 14324,  2003,  2367,  2138,  2009,\n",
              "           103,  2881,  2000,  3191,  1999,  2119,  7826,  2012,  2320,  1012,\n",
              "           103,   103,  1010,  9124,  2011,  1996,  4955,  1997, 19081,  1010,\n",
              "          2003,  2124,  2004,  7226,  7442,  7542, 23732,  1012,  2478,   103,\n",
              "          7226,  7442,  7542,  2389, 10673,  1010,   103,   103,  3653,  1011,\n",
              "          4738,  2006,  2048,  2367,   103,  2021,   103,  1010, 17953,  2361,\n",
              "          8518,  1024,   103,  2653,   103,  1998,  2279,  6251, 17547,   103,\n",
              "          1996,  7863,  1997, 16520,  2653,  2944,   103, 19875,  2213,  1007,\n",
              "          2731,  2003,  2000,  5342,  1037,  2773,   103,  1037,  6251,  1998,\n",
              "          2059,  2031,  1996,   103,   103,  2054,  2773,  2038,  2042,   103,\n",
              "          1006, 16520,  1007,   103,  2006,  1996,  5023,  2773,  1005,  1055,\n",
              "           103,  1012,  1996,  7863,  1997,  2279,  6251,   103,  2731,  2003,\n",
              "          2000,  2031,  1996,  2565, 16014,  3251,  2048,  2445, 11746,  2031,\n",
              "          1037,   103,  1010, 25582,  4434,   103,   103,  2037,  3276,  2003,\n",
              "          3432,  6721,  1012,   102]])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = model(**inputs)\n",
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkH2FQnKGlbl",
        "outputId": "08a5faf5-6544-4888-abdf-5476dc463aae"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.5344, -0.6189, -0.2586,  ...,  0.4400, -0.4150,  0.7122],\n",
              "         [ 0.6019, -0.0820,  0.1610,  ...,  0.5706,  0.5555, -0.1386],\n",
              "         [-0.8314, -0.2836, -0.1674,  ..., -0.0136, -0.7717,  0.9642],\n",
              "         ...,\n",
              "         [ 0.5522, -0.8950,  0.0989,  ..., -0.4885, -0.5068, -0.3868],\n",
              "         [-0.2382, -0.0602, -0.2001,  ...,  0.2692,  0.0392, -0.6805],\n",
              "         [-0.4988,  0.1139,  0.3879,  ...,  0.6775, -0.3838,  0.1073]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.1268,  0.4925,  0.9304, -0.6846, -0.3763,  0.3031, -0.9780, -0.5257,\n",
              "          0.8248, -0.2527,  0.6266, -0.9237,  0.6398, -0.9127, -0.2507,  0.5358,\n",
              "          0.8077,  0.3497, -0.2207,  0.9708, -0.6258, -0.9809,  0.6889, -0.1542,\n",
              "         -0.4968, -0.9893,  0.4781,  0.2413,  0.3579,  0.3581,  0.6038, -0.6049,\n",
              "         -0.7799,  0.4752,  0.7407,  0.0281, -0.3862,  0.1716,  0.5339,  0.5208,\n",
              "         -0.2749, -0.2529,  0.8422, -0.9749, -0.3167,  0.3272,  1.0000, -0.2530,\n",
              "         -0.1670, -0.9077, -0.8805, -0.7955, -0.4057, -0.5703, -0.3298,  0.0900,\n",
              "         -0.7703, -0.5747,  0.2097,  0.4234,  0.0941, -0.5936,  0.8742, -0.1369,\n",
              "         -0.9265, -0.9110,  0.4503,  0.4621,  0.4311, -0.4377, -0.5561, -0.5174,\n",
              "          0.5301, -0.1609, -0.8875, -0.0997, -0.9112, -1.0000,  0.6485, -0.6529,\n",
              "         -0.8356, -0.8892,  0.8699,  0.8757, -0.9352,  1.0000, -0.2592,  0.4583,\n",
              "         -0.6148, -0.3390, -0.3314,  0.4457, -0.8195,  0.9029,  0.8445,  0.3556,\n",
              "          0.6026,  0.6041,  0.4966,  0.3417, -0.3354,  0.4898,  0.4733,  0.0208,\n",
              "         -0.0698,  0.4205,  0.8956, -0.6606, -0.7782, -0.5610,  0.8650,  0.2932,\n",
              "         -0.5325, -0.1643, -0.4282,  0.2466, -0.7916, -0.9015, -0.4636, -0.1557,\n",
              "          0.8688,  0.2096,  0.1794,  0.8539, -0.3365,  0.3877,  0.8761,  1.0000,\n",
              "          0.6801,  0.5248,  0.7832,  0.6035, -0.4979, -0.5835, -0.4444,  0.7038,\n",
              "         -0.3482,  0.3309,  0.5799,  0.2912,  0.9282,  0.3150, -0.7154,  0.5660,\n",
              "         -0.3851, -0.9238,  0.7988, -0.2154,  0.5095, -0.6903,  0.7800,  0.2611,\n",
              "          0.7740,  0.2690,  0.1846,  0.4669,  0.4872,  0.9080,  0.8797,  0.6083,\n",
              "          0.7127,  0.2237, -0.3476, -0.5587,  0.7932, -0.3689, -0.9873, -0.5271,\n",
              "          0.3091,  0.6915, -0.3666,  0.3275,  0.8344, -0.7211,  0.4776, -0.6681,\n",
              "          0.5050, -0.5267, -0.3440, -0.8698,  0.0432, -0.6124, -0.5338,  0.7293,\n",
              "          0.7350, -0.2789,  0.3958, -0.4720, -0.3640, -0.8312, -0.7142,  0.6961,\n",
              "         -0.5656,  0.5715, -0.1201,  0.4717,  0.7840, -0.0836,  0.3196,  0.3301,\n",
              "          0.6619,  0.5469, -0.8977, -0.5472, -0.6178,  0.0036,  0.3558,  0.7806,\n",
              "         -0.4667, -0.4183, -0.5247,  0.5978, -0.0252,  0.5665, -0.3889, -0.4464,\n",
              "         -0.5607, -0.0282,  0.5036,  0.3596, -0.5944,  0.0161, -0.9521, -0.6701,\n",
              "          0.9942,  0.3793, -0.5657, -0.0403, -0.2564,  0.3354,  0.9286, -0.8506,\n",
              "          0.2454, -0.9823,  0.8965,  0.8384, -0.6640, -0.9287,  0.3256, -0.3676,\n",
              "          0.4413,  0.3005, -0.8424, -0.5853,  0.1574,  0.7995,  0.2083,  0.9579,\n",
              "          0.4361, -0.9843, -0.8986,  0.9234, -0.4172,  0.9690,  0.5624, -0.9995,\n",
              "         -0.8381, -0.6325, -0.8837,  0.8973, -0.8238, -0.8092, -0.2305, -0.4029,\n",
              "          0.3330,  0.5290, -0.5438, -0.8532, -0.5639, -0.9189, -0.0151,  0.9154,\n",
              "          0.1875, -0.4082, -0.5925,  0.5417,  0.8915, -0.5159, -0.3176, -0.1743,\n",
              "          0.2838, -0.4967,  0.8768, -0.9971, -0.9998,  0.5299,  0.1104, -0.9088,\n",
              "          0.9524, -0.8788,  0.3041,  0.8613,  1.0000, -0.4674,  0.4775, -0.4118,\n",
              "          1.0000,  0.0899,  0.6766,  0.0161, -0.8601,  0.4620, -0.9977,  1.0000,\n",
              "         -0.0058,  0.3145, -0.9248, -0.9479,  0.2569,  0.5603, -0.6650,  0.2480,\n",
              "          0.4601, -0.5173,  0.8879,  0.7499,  0.8464,  0.4973, -0.7973, -0.4587,\n",
              "          0.5398,  0.3816, -0.4737,  0.6475, -0.7985, -0.4732,  0.2914, -0.8834,\n",
              "         -0.5557, -0.3295, -0.2810,  0.3992, -0.1769, -0.7408, -0.7073,  0.8836,\n",
              "          0.1173,  0.6453,  0.9433, -0.6666,  0.9255, -0.8329,  0.3159,  0.5610,\n",
              "         -0.8737, -1.0000,  0.2193,  0.5355, -0.5384, -0.6006,  0.7436, -1.0000,\n",
              "         -0.6306, -0.7137, -0.9197, -0.4195,  0.4430,  0.3084,  0.1707,  0.3017,\n",
              "          0.6671,  0.6147,  0.8052, -0.7532, -0.8334,  0.6467, -0.5125,  0.5149,\n",
              "          0.3145,  0.6069, -0.3652, -0.6005,  0.9593, -0.3933,  0.3692, -0.7475,\n",
              "         -0.5419,  0.4044,  0.0912,  0.9775, -0.9233, -0.3850,  0.5138,  0.8520,\n",
              "         -0.7726, -0.5193,  0.0422, -0.5302,  0.5382, -0.6312, -0.4758, -0.3487,\n",
              "         -1.0000,  0.7204, -0.6563,  0.9059,  0.3926, -0.6389, -0.0610, -0.2338,\n",
              "          0.5124,  0.3479, -0.5939,  0.9073, -0.7824, -0.6691, -0.5846,  0.6174,\n",
              "         -0.9999, -0.6681, -0.4653, -0.4313, -0.9501, -0.4719, -0.7352, -0.9435,\n",
              "          0.7998,  0.3779,  0.9323, -0.8630,  0.9594,  0.4344,  0.4101, -0.6863,\n",
              "         -0.6139,  0.6483, -0.1756,  0.6533, -0.8092, -0.4460, -0.4459, -0.4664,\n",
              "         -1.0000, -0.9749, -0.6147,  0.9881, -0.9788,  0.9017,  0.8078,  0.5359,\n",
              "          0.3732,  0.9268,  0.4225, -0.5535, -0.4862, -0.8886, -0.5982,  0.9623,\n",
              "         -0.2830,  0.7159, -0.8430, -0.4281,  0.8980,  0.8581, -0.0373, -0.7703,\n",
              "          0.3300,  0.0722,  0.8397,  0.1717, -0.3237,  0.3390,  0.8893, -0.7960,\n",
              "          0.9926, -0.1572,  0.3961,  0.5668,  0.6170, -0.5228,  0.4059,  0.9108,\n",
              "          0.2771, -1.0000,  0.7023, -0.7971, -0.3663, -0.6320,  0.3823, -0.2114,\n",
              "         -0.7757, -0.4342,  0.9804,  0.8224,  0.9965,  0.2951, -0.3015, -0.4637,\n",
              "         -0.9394, -0.1253, -0.0534, -0.6277,  0.6095, -0.5267, -0.9998,  0.4893,\n",
              "          0.5765,  0.4011,  0.5235,  0.4906,  0.9785, -1.0000, -0.5027, -0.1541,\n",
              "         -0.7290,  0.7031,  0.4646, -0.9986,  0.5717, -0.1894, -0.5211, -0.6144,\n",
              "          0.3844, -0.8510,  0.4468,  0.4100, -0.5403, -0.3299,  0.3108,  0.6038,\n",
              "         -0.7330, -0.2142, -0.5724, -0.5710, -0.9591,  0.3958,  0.4146,  0.5120,\n",
              "          0.3873, -0.9989, -0.7146, -0.4765,  0.5343, -0.3563,  0.4039,  0.0416,\n",
              "         -0.9094, -0.4604, -0.6929, -0.4846,  0.8659, -0.5119,  0.1938, -0.1020,\n",
              "         -0.4868, -1.0000,  0.2651, -0.7672, -0.5927,  0.5179, -0.4804,  0.5851,\n",
              "         -0.6957, -0.5297,  0.5192,  0.4878, -0.3608,  0.5391, -0.4082, -0.2587,\n",
              "         -0.3815, -0.9041,  0.4962,  0.4636, -0.1994, -0.7638,  0.2441,  0.5048,\n",
              "         -0.4624,  0.4715,  0.7829,  0.5420,  0.1720,  0.9750, -0.3162,  1.0000,\n",
              "         -0.5503, -0.9527,  0.3635,  0.1083,  0.6388, -0.3199,  0.3022,  0.8330,\n",
              "          0.9588, -0.2648,  0.5016,  0.7991,  0.2009, -0.4487,  0.7101, -0.5742,\n",
              "          0.7032,  0.3614,  0.2395, -1.0000, -0.5497,  0.3240,  0.9836, -0.4172,\n",
              "          0.2977, -0.9999,  0.9316, -0.7102, -0.4165,  0.3876,  0.3476, -0.0824,\n",
              "         -0.1976,  0.3397,  0.9142, -0.4393, -0.8434, -0.9035, -0.1555,  0.6346,\n",
              "          0.4548, -0.5327, -0.6375,  0.6137, -0.4138, -0.5746, -0.4929,  0.5915,\n",
              "          0.7333, -0.3206, -0.8822, -0.5590, -1.0000, -0.2997, -0.1632,  0.8989,\n",
              "         -0.0678,  0.5457, -0.3864, -0.3302, -0.5122,  0.6331,  0.4236,  0.0033,\n",
              "          0.7308, -0.5033,  0.4860,  0.8824, -0.2116, -0.2653, -0.6997, -0.5968,\n",
              "         -0.5153,  0.1205,  0.2304, -0.2846, -0.4112, -0.4267, -0.6135,  1.0000,\n",
              "          0.5755, -0.4251, -0.9342,  0.1551, -0.6330, -0.0982, -0.2249, -0.6361,\n",
              "          0.9961,  0.6384,  0.2692, -0.6720, -0.2996, -0.0534, -0.6226,  0.2464,\n",
              "          0.0672,  0.8433, -0.8942, -0.7545, -0.3030,  0.8445,  0.9840,  0.5230,\n",
              "         -0.9529,  0.3053,  0.9243,  0.6046, -0.9765, -0.3675, -0.7208, -0.6396,\n",
              "          0.0039, -0.2051, -0.4030,  0.5995,  0.8266, -0.4255,  0.5881, -0.5780,\n",
              "          0.9367, -0.8268, -0.2609, -0.2223,  0.8442, -0.3486,  0.4221,  0.9828,\n",
              "          0.8530,  0.3863, -0.6334,  0.3770,  0.6552,  0.2259, -0.0083, -0.2015,\n",
              "          0.6709,  0.8348, -0.7486,  0.1905, -0.4791,  0.4259,  0.7032,  0.5546,\n",
              "          0.9526, -0.5470, -0.9413,  0.7036,  0.9670, -0.3772, -0.6879,  0.9586,\n",
              "         -0.7967,  0.4653,  0.5177,  0.3080, -0.5490, -0.6900,  0.3470,  0.5971,\n",
              "         -0.5932, -0.2871, -0.9786,  0.8489,  0.9497, -0.0915,  0.6439, -0.1044,\n",
              "          0.4310,  1.0000, -0.3537,  0.7249, -0.8241,  0.7867, -0.8159,  0.6248,\n",
              "          0.7784,  0.5470,  0.1112, -0.3660,  0.2669,  0.6071,  0.8751,  0.6959,\n",
              "         -0.8372, -0.1843, -0.2439,  0.8981,  0.9057,  0.1735,  0.4856, -0.2937]],\n",
              "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Gk_u5dmL4qZ",
        "outputId": "2a4d2cc0-53c1-4e4b-cdf8-789b79b257e7"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys(['last_hidden_state', 'pooler_output'])"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.last_hidden_state"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wRzD8A6MCEa",
        "outputId": "d4560a99-adfc-4cbb-88d3-259201dcbd5f"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.5344, -0.6189, -0.2586,  ...,  0.4400, -0.4150,  0.7122],\n",
              "         [ 0.6019, -0.0820,  0.1610,  ...,  0.5706,  0.5555, -0.1386],\n",
              "         [-0.8314, -0.2836, -0.1674,  ..., -0.0136, -0.7717,  0.9642],\n",
              "         ...,\n",
              "         [ 0.5522, -0.8950,  0.0989,  ..., -0.4885, -0.5068, -0.3868],\n",
              "         [-0.2382, -0.0602, -0.2001,  ...,  0.2692,  0.0392, -0.6805],\n",
              "         [-0.4988,  0.1139,  0.3879,  ...,  0.6775, -0.3838,  0.1073]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs.pooler_output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-D6q--KMIJC",
        "outputId": "0a3d4331-040d-4c8f-b5e6-9692a6402cc0"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1268,  0.4925,  0.9304, -0.6846, -0.3763,  0.3031, -0.9780, -0.5257,\n",
              "          0.8248, -0.2527,  0.6266, -0.9237,  0.6398, -0.9127, -0.2507,  0.5358,\n",
              "          0.8077,  0.3497, -0.2207,  0.9708, -0.6258, -0.9809,  0.6889, -0.1542,\n",
              "         -0.4968, -0.9893,  0.4781,  0.2413,  0.3579,  0.3581,  0.6038, -0.6049,\n",
              "         -0.7799,  0.4752,  0.7407,  0.0281, -0.3862,  0.1716,  0.5339,  0.5208,\n",
              "         -0.2749, -0.2529,  0.8422, -0.9749, -0.3167,  0.3272,  1.0000, -0.2530,\n",
              "         -0.1670, -0.9077, -0.8805, -0.7955, -0.4057, -0.5703, -0.3298,  0.0900,\n",
              "         -0.7703, -0.5747,  0.2097,  0.4234,  0.0941, -0.5936,  0.8742, -0.1369,\n",
              "         -0.9265, -0.9110,  0.4503,  0.4621,  0.4311, -0.4377, -0.5561, -0.5174,\n",
              "          0.5301, -0.1609, -0.8875, -0.0997, -0.9112, -1.0000,  0.6485, -0.6529,\n",
              "         -0.8356, -0.8892,  0.8699,  0.8757, -0.9352,  1.0000, -0.2592,  0.4583,\n",
              "         -0.6148, -0.3390, -0.3314,  0.4457, -0.8195,  0.9029,  0.8445,  0.3556,\n",
              "          0.6026,  0.6041,  0.4966,  0.3417, -0.3354,  0.4898,  0.4733,  0.0208,\n",
              "         -0.0698,  0.4205,  0.8956, -0.6606, -0.7782, -0.5610,  0.8650,  0.2932,\n",
              "         -0.5325, -0.1643, -0.4282,  0.2466, -0.7916, -0.9015, -0.4636, -0.1557,\n",
              "          0.8688,  0.2096,  0.1794,  0.8539, -0.3365,  0.3877,  0.8761,  1.0000,\n",
              "          0.6801,  0.5248,  0.7832,  0.6035, -0.4979, -0.5835, -0.4444,  0.7038,\n",
              "         -0.3482,  0.3309,  0.5799,  0.2912,  0.9282,  0.3150, -0.7154,  0.5660,\n",
              "         -0.3851, -0.9238,  0.7988, -0.2154,  0.5095, -0.6903,  0.7800,  0.2611,\n",
              "          0.7740,  0.2690,  0.1846,  0.4669,  0.4872,  0.9080,  0.8797,  0.6083,\n",
              "          0.7127,  0.2237, -0.3476, -0.5587,  0.7932, -0.3689, -0.9873, -0.5271,\n",
              "          0.3091,  0.6915, -0.3666,  0.3275,  0.8344, -0.7211,  0.4776, -0.6681,\n",
              "          0.5050, -0.5267, -0.3440, -0.8698,  0.0432, -0.6124, -0.5338,  0.7293,\n",
              "          0.7350, -0.2789,  0.3958, -0.4720, -0.3640, -0.8312, -0.7142,  0.6961,\n",
              "         -0.5656,  0.5715, -0.1201,  0.4717,  0.7840, -0.0836,  0.3196,  0.3301,\n",
              "          0.6619,  0.5469, -0.8977, -0.5472, -0.6178,  0.0036,  0.3558,  0.7806,\n",
              "         -0.4667, -0.4183, -0.5247,  0.5978, -0.0252,  0.5665, -0.3889, -0.4464,\n",
              "         -0.5607, -0.0282,  0.5036,  0.3596, -0.5944,  0.0161, -0.9521, -0.6701,\n",
              "          0.9942,  0.3793, -0.5657, -0.0403, -0.2564,  0.3354,  0.9286, -0.8506,\n",
              "          0.2454, -0.9823,  0.8965,  0.8384, -0.6640, -0.9287,  0.3256, -0.3676,\n",
              "          0.4413,  0.3005, -0.8424, -0.5853,  0.1574,  0.7995,  0.2083,  0.9579,\n",
              "          0.4361, -0.9843, -0.8986,  0.9234, -0.4172,  0.9690,  0.5624, -0.9995,\n",
              "         -0.8381, -0.6325, -0.8837,  0.8973, -0.8238, -0.8092, -0.2305, -0.4029,\n",
              "          0.3330,  0.5290, -0.5438, -0.8532, -0.5639, -0.9189, -0.0151,  0.9154,\n",
              "          0.1875, -0.4082, -0.5925,  0.5417,  0.8915, -0.5159, -0.3176, -0.1743,\n",
              "          0.2838, -0.4967,  0.8768, -0.9971, -0.9998,  0.5299,  0.1104, -0.9088,\n",
              "          0.9524, -0.8788,  0.3041,  0.8613,  1.0000, -0.4674,  0.4775, -0.4118,\n",
              "          1.0000,  0.0899,  0.6766,  0.0161, -0.8601,  0.4620, -0.9977,  1.0000,\n",
              "         -0.0058,  0.3145, -0.9248, -0.9479,  0.2569,  0.5603, -0.6650,  0.2480,\n",
              "          0.4601, -0.5173,  0.8879,  0.7499,  0.8464,  0.4973, -0.7973, -0.4587,\n",
              "          0.5398,  0.3816, -0.4737,  0.6475, -0.7985, -0.4732,  0.2914, -0.8834,\n",
              "         -0.5557, -0.3295, -0.2810,  0.3992, -0.1769, -0.7408, -0.7073,  0.8836,\n",
              "          0.1173,  0.6453,  0.9433, -0.6666,  0.9255, -0.8329,  0.3159,  0.5610,\n",
              "         -0.8737, -1.0000,  0.2193,  0.5355, -0.5384, -0.6006,  0.7436, -1.0000,\n",
              "         -0.6306, -0.7137, -0.9197, -0.4195,  0.4430,  0.3084,  0.1707,  0.3017,\n",
              "          0.6671,  0.6147,  0.8052, -0.7532, -0.8334,  0.6467, -0.5125,  0.5149,\n",
              "          0.3145,  0.6069, -0.3652, -0.6005,  0.9593, -0.3933,  0.3692, -0.7475,\n",
              "         -0.5419,  0.4044,  0.0912,  0.9775, -0.9233, -0.3850,  0.5138,  0.8520,\n",
              "         -0.7726, -0.5193,  0.0422, -0.5302,  0.5382, -0.6312, -0.4758, -0.3487,\n",
              "         -1.0000,  0.7204, -0.6563,  0.9059,  0.3926, -0.6389, -0.0610, -0.2338,\n",
              "          0.5124,  0.3479, -0.5939,  0.9073, -0.7824, -0.6691, -0.5846,  0.6174,\n",
              "         -0.9999, -0.6681, -0.4653, -0.4313, -0.9501, -0.4719, -0.7352, -0.9435,\n",
              "          0.7998,  0.3779,  0.9323, -0.8630,  0.9594,  0.4344,  0.4101, -0.6863,\n",
              "         -0.6139,  0.6483, -0.1756,  0.6533, -0.8092, -0.4460, -0.4459, -0.4664,\n",
              "         -1.0000, -0.9749, -0.6147,  0.9881, -0.9788,  0.9017,  0.8078,  0.5359,\n",
              "          0.3732,  0.9268,  0.4225, -0.5535, -0.4862, -0.8886, -0.5982,  0.9623,\n",
              "         -0.2830,  0.7159, -0.8430, -0.4281,  0.8980,  0.8581, -0.0373, -0.7703,\n",
              "          0.3300,  0.0722,  0.8397,  0.1717, -0.3237,  0.3390,  0.8893, -0.7960,\n",
              "          0.9926, -0.1572,  0.3961,  0.5668,  0.6170, -0.5228,  0.4059,  0.9108,\n",
              "          0.2771, -1.0000,  0.7023, -0.7971, -0.3663, -0.6320,  0.3823, -0.2114,\n",
              "         -0.7757, -0.4342,  0.9804,  0.8224,  0.9965,  0.2951, -0.3015, -0.4637,\n",
              "         -0.9394, -0.1253, -0.0534, -0.6277,  0.6095, -0.5267, -0.9998,  0.4893,\n",
              "          0.5765,  0.4011,  0.5235,  0.4906,  0.9785, -1.0000, -0.5027, -0.1541,\n",
              "         -0.7290,  0.7031,  0.4646, -0.9986,  0.5717, -0.1894, -0.5211, -0.6144,\n",
              "          0.3844, -0.8510,  0.4468,  0.4100, -0.5403, -0.3299,  0.3108,  0.6038,\n",
              "         -0.7330, -0.2142, -0.5724, -0.5710, -0.9591,  0.3958,  0.4146,  0.5120,\n",
              "          0.3873, -0.9989, -0.7146, -0.4765,  0.5343, -0.3563,  0.4039,  0.0416,\n",
              "         -0.9094, -0.4604, -0.6929, -0.4846,  0.8659, -0.5119,  0.1938, -0.1020,\n",
              "         -0.4868, -1.0000,  0.2651, -0.7672, -0.5927,  0.5179, -0.4804,  0.5851,\n",
              "         -0.6957, -0.5297,  0.5192,  0.4878, -0.3608,  0.5391, -0.4082, -0.2587,\n",
              "         -0.3815, -0.9041,  0.4962,  0.4636, -0.1994, -0.7638,  0.2441,  0.5048,\n",
              "         -0.4624,  0.4715,  0.7829,  0.5420,  0.1720,  0.9750, -0.3162,  1.0000,\n",
              "         -0.5503, -0.9527,  0.3635,  0.1083,  0.6388, -0.3199,  0.3022,  0.8330,\n",
              "          0.9588, -0.2648,  0.5016,  0.7991,  0.2009, -0.4487,  0.7101, -0.5742,\n",
              "          0.7032,  0.3614,  0.2395, -1.0000, -0.5497,  0.3240,  0.9836, -0.4172,\n",
              "          0.2977, -0.9999,  0.9316, -0.7102, -0.4165,  0.3876,  0.3476, -0.0824,\n",
              "         -0.1976,  0.3397,  0.9142, -0.4393, -0.8434, -0.9035, -0.1555,  0.6346,\n",
              "          0.4548, -0.5327, -0.6375,  0.6137, -0.4138, -0.5746, -0.4929,  0.5915,\n",
              "          0.7333, -0.3206, -0.8822, -0.5590, -1.0000, -0.2997, -0.1632,  0.8989,\n",
              "         -0.0678,  0.5457, -0.3864, -0.3302, -0.5122,  0.6331,  0.4236,  0.0033,\n",
              "          0.7308, -0.5033,  0.4860,  0.8824, -0.2116, -0.2653, -0.6997, -0.5968,\n",
              "         -0.5153,  0.1205,  0.2304, -0.2846, -0.4112, -0.4267, -0.6135,  1.0000,\n",
              "          0.5755, -0.4251, -0.9342,  0.1551, -0.6330, -0.0982, -0.2249, -0.6361,\n",
              "          0.9961,  0.6384,  0.2692, -0.6720, -0.2996, -0.0534, -0.6226,  0.2464,\n",
              "          0.0672,  0.8433, -0.8942, -0.7545, -0.3030,  0.8445,  0.9840,  0.5230,\n",
              "         -0.9529,  0.3053,  0.9243,  0.6046, -0.9765, -0.3675, -0.7208, -0.6396,\n",
              "          0.0039, -0.2051, -0.4030,  0.5995,  0.8266, -0.4255,  0.5881, -0.5780,\n",
              "          0.9367, -0.8268, -0.2609, -0.2223,  0.8442, -0.3486,  0.4221,  0.9828,\n",
              "          0.8530,  0.3863, -0.6334,  0.3770,  0.6552,  0.2259, -0.0083, -0.2015,\n",
              "          0.6709,  0.8348, -0.7486,  0.1905, -0.4791,  0.4259,  0.7032,  0.5546,\n",
              "          0.9526, -0.5470, -0.9413,  0.7036,  0.9670, -0.3772, -0.6879,  0.9586,\n",
              "         -0.7967,  0.4653,  0.5177,  0.3080, -0.5490, -0.6900,  0.3470,  0.5971,\n",
              "         -0.5932, -0.2871, -0.9786,  0.8489,  0.9497, -0.0915,  0.6439, -0.1044,\n",
              "          0.4310,  1.0000, -0.3537,  0.7249, -0.8241,  0.7867, -0.8159,  0.6248,\n",
              "          0.7784,  0.5470,  0.1112, -0.3660,  0.2669,  0.6071,  0.8751,  0.6959,\n",
              "         -0.8372, -0.1843, -0.2439,  0.8981,  0.9057,  0.1735,  0.4856, -0.2937]],\n",
              "       grad_fn=<TanhBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oeNV_D35SdFw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}